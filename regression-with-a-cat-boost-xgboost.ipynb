{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":73278,"databundleVersionId":8121328,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Steps for this Notebook:\n\n1. **üîç Data Loading and Exploration:**\n   - Load the training, testing, and sample submission datasets üìä.\n   - Explore the structure and basic statistics of the datasets üìà.\n   - Visualize the distribution of numerical features using boxplots üìä.\n   - Identify potential outliers in the data üö®.\n\n2. **üõ†Ô∏è Data Preprocessing:**\n   - Check for missing values and handle them appropriately if necessary üßπ.\n   - Examine the correlation between features and the target variable üîç.\n   - Calculate the Variance Inflation Factor (VIF) to detect multicollinearity üìä.\n   - Perform any necessary feature engineering, such as creating interaction terms or scaling features üõ†Ô∏è.\n\n3. **ü§ñ Model Training and Evaluation:**\n   - Define the configuration settings for the models, such as the number of splits for cross-validation ‚öôÔ∏è.\n   - Split the training data into features (X) and the target variable (y) üìâ.\n   - Define a custom feature engineering class to preprocess the data before model training üß∞.\n   - Implement a function to score the models using cross-validation and print the evaluation metrics for each fold üìù.\n   - Train and evaluate XGBoost, CatBoost, and Linear Regression models using the defined pipeline üöÄ.\n   - Visualize feature importance for XGBoost and CatBoost models üåü.\n   - Plot scatter plots, residual plots, and histograms of residuals for model evaluation üìä.\n\n4. **üìë Submission Preparation:**\n   - Make predictions on the test dataset using the trained models üéØ.\n   - Create a submission DataFrame with the predicted values and the corresponding IDs üíº.\n   - Save the submission DataFrame to a CSV file for submission to the competition üìÑ.\n\n### Notebook Description:\n\nThis notebook takes you on an exciting journey through the world of flood prediction competition üåä. Starting with data loading and exploration, you'll dive deep into understanding the datasets and visualizing key insights. \n\nAfter preprocessing the data to ensure it's primed for modeling, you'll embark on a thrilling adventure of model training and evaluation. With XGBoost, CatBoost, and Linear Regression models as your trusty companions, you'll traverse the landscape of feature engineering and cross-validation, uncovering hidden patterns and insights along the way.\n\nWith feature importance as your guiding star, you'll chart a course towards model evaluation, navigating through scatter plots, residual plots, and histograms to ensure your models are battle-ready. \n\nFinally, armed with the knowledge gained from your exploration and the power of your models, you'll prepare your submission, ready to conquer the leaderboard and emerge victorious in the flood prediction competition üèÜ.\n\n[Competition Link](https://www.kaggle.com/competitions/playground-series-s4e5/overview)","metadata":{}},{"cell_type":"code","source":"# Import necessary libraries\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import BaseEstimator, TransformerMixin, clone\nfrom sklearn.metrics import r2_score\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data reading\ndf_train = pd.read_csv(r'/kaggle/input/playground-series-s4e5/train.csv')\ndf_test = pd.read_csv(r'/kaggle/input/playground-series-s4e5/test.csv')\ndf_sub = pd.read_csv(r'/kaggle/input/playground-series-s4e5/sample_submission.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"desc = pd.DataFrame(index = list(df_train))\ndesc['type'] = df_train.dtypes\ndesc['count'] = df_train.count()\ndesc['nunique'] = df_train.nunique()\ndesc['%unique'] = desc['nunique'] /len(df_train) * 100\ndesc['null'] = df_train.isnull().sum()\ndesc['%null'] = desc['null'] / len(df_train) * 100\ndesc = pd.concat([desc,df_train.describe().T.drop('count',axis=1)],axis=1)\ndesc.sort_values(by=['type','null']).style.background_gradient(axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.info()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Ouliers","metadata":{}},{"cell_type":"code","source":"numeric_features = df_train.select_dtypes(include=['int64', 'float64']).columns.drop('id')\nplt.figure(figsize=(12, 8))\nsns.boxplot(data=df_train[numeric_features], orient=\"h\", palette=\"Set2\")\nplt.title('Boxplot of Numerical Features')\nplt.xlabel('Value')\nplt.gcf().set_facecolor('#008080')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to calculate the number of outliers for each column\ndef count_outliers(df, columns):\n    outlier_counts = {}\n    for col in columns:\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n        outlier_counts[col] = outliers.shape[0]\n    return outlier_counts\n\n# List of columns to check for outliers (excluding the target variable)\ncolumns = df_train.columns\n\n# Count the outliers\noutlier_counts = count_outliers(df_train, columns)\n\n# Print the counts of outliers for each column\nfor col, count in outlier_counts.items():\n    print(f'{col}: {count} outliers')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Linearity","metadata":{}},{"cell_type":"markdown","source":"*In order to assess whether or not there is a linear relationship between the independent and dependent variables, it is easiest to create a scatterplot of the dataset. The independent variable would be on the x-axis, and the dependent variable would be on the y-axis. There are a number of different Python functions that you can use to read in the data and to create a scatterplot*","metadata":{}},{"cell_type":"code","source":"# Scatter plots for each feature vs. the target variable\nfor col in df_train.columns:\n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=df_train[col], y=df_train['FloodProbability'])\n    plt.title(f'Scatter plot of {col} vs FloodProbability')\n    plt.xlabel(col)\n    plt.ylabel('FloodProbability')\n    plt.show()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Target Distrpution\ndf_train.hist(figsize=(15,15));","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Correlation\ncorr = df_train.corr()\n# Plot the correlation matrix with a larger size and increased cell spacing\nplt.figure(figsize=(14, 12))\nsns.heatmap(corr, cmap=\"Blues\", annot=True, fmt=\".2f\", linewidths=0.5, square=True, cbar_kws={'label': 'Correlation', 'orientation': 'vertical'})\n\nplt.title('Correlation Matrix', fontsize=16)\nplt.xticks(rotation=45, ha='right')\nplt.yticks(rotation=0)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking Collinearity","metadata":{}},{"cell_type":"markdown","source":"**What is VIF?**\n\nVariance Inflation Factor (VIF) is a measure used to detect the presence and severity of multicollinearity in regression analysis. Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, meaning that one predictor can be linearly predicted from the others with a substantial degree of accuracy.\n\n\n1. VIF = 1: No correlation between the predictor and the other predictors.\n\n2. 1 < VIF < 5: Moderate correlation but usually acceptable.\n\n3. VIF > 5: Indicates potentially problematic multicollinearity.\n\n4. VIF > 10: Indicates serious multicollinearity.","metadata":{}},{"cell_type":"code","source":"df_train_with_const = add_constant(df_train) \n\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = df_train_with_const.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(df_train_with_const.values, i) for i in range(df_train_with_const.shape[1])]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vif_data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test.isnull().sum()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelling","metadata":{}},{"cell_type":"markdown","source":"**Step-by-Step Integration**\n\n**1. Define Configuration and Setup**\n\n**2. Define Feature Engineering Class**\n\n**3. Define Model Scoring Function**\n\n**4. Run the Model and Evaluate**","metadata":{}},{"cell_type":"code","source":"# Define the configuration\nconfig = {\n    'N_SPLITS': 5,\n    'SEED': 42,\n    'USE_ORIGINAL': False  # Set to True if using additional original data\n}\n\nTARGET = 'FloodProbability'\n\n# Assuming df_train is your DataFrame and df_test is your test set\nX = df_train.drop(columns=[TARGET])\ny = df_train[TARGET]\ntest = df_test  # Assuming df_test is already defined","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Feature Engineering Class\nclass FE(BaseEstimator, TransformerMixin):\n    def __init__(self, use_quantile=False, sort=False):\n        self.use_quantile = use_quantile\n        self.sort = sort\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        x_copy = X.copy()\n        features = x_copy.columns.tolist()\n        x_copy['mean_features'] = 0.1 * x_copy[features].mean(axis=1)\n        x_copy['std_features'] = x_copy[features].std(axis=1)\n        x_copy['max_features'] = x_copy[features].max(axis=1)\n        x_copy['min_features'] = x_copy[features].min(axis=1)\n        x_copy['median_features'] = 0.1 * x_copy[features].median(axis=1)\n        x_copy['sum_features'] = x_copy[features].sum(axis=1)\n        \n        if self.sort:\n            sorted_values = np.sort(x_copy[features].values, axis=1)\n            sorted_features = [f'sort_{i}' for i in range(sorted_values.shape[1])]\n            sorted_df = pd.DataFrame(sorted_values, columns=sorted_features, index=x_copy.index)\n            x_copy = pd.concat([x_copy, sorted_df], axis=1)\n        \n        if self.use_quantile:\n            x_copy['q1'] = x_copy[features].quantile(0.25, axis=1)\n            x_copy['q3'] = x_copy[features].quantile(0.75, axis=1)\n        \n        return x_copy.drop(features, axis=1)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define Model Scoring Function \nkf = KFold(n_splits=config['N_SPLITS'], random_state=config['SEED'], shuffle=True)\n\ndef score_model(estimator, label=''):\n    val_predictions = np.zeros((len(X)))\n    test_predictions = np.zeros((len(test)))\n    val_scores = []  \n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        model = clone(estimator)\n\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)\n\n        model.fit(X_train, y_train)      \n        val_preds = model.predict(X_val)\n        val_predictions[val_idx] += val_preds\n        test_predictions += model.predict(test) / kf.get_n_splits()\n                \n        val_score = r2_score(y_val, val_preds)\n        val_scores.append(val_score)\n\n        print(f'Fold {fold+1}: {val_score:.5f}')\n    \n    print(f'Val Score: {np.mean(val_scores):.5f} ¬± {np.std(val_scores):.5f} | {label}')\n\n    return val_scores, val_predictions, test_predictions\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Xgboost","metadata":{}},{"cell_type":"code","source":"# Run the Model and Evaluate\nscores, oof, test_preds = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n\n# Train and evaluate the XGBoost model\nscores['XGB'], oof['XGB'], test_preds['XGB'] = score_model(\n    make_pipeline(\n        FE(use_quantile=True, sort=True),\n        xgb.XGBRegressor(\n            n_estimators=600,\n            max_depth=4,\n            learning_rate=0.04,\n            objective='reg:squarederror',\n            random_state=config['SEED']\n        )\n    ),\n    'XGB'\n)\n\n# Visualize feature importance\nmodel = xgb.XGBRegressor(\n    n_estimators=600,\n    max_depth=4,\n    learning_rate=0.04,\n    objective='reg:squarederror',\n    random_state=config['SEED']\n)\nmodel.fit(X, y)\nxgb.plot_importance(model, max_num_features=10, importance_type='gain')\nplt.title('Feature Importance by Gain')\nplt.show()\n\n# Custom feature importance plot\nimportance = model.feature_importances_\nfeature_names = X.columns\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance}).sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importance')\nplt.show()\n\n# Scatter plot of predicted vs actual values\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y, y=model.predict(X))\nplt.xlabel('Actual FloodProbability')\nplt.ylabel('Predicted FloodProbability')\nplt.title('Actual vs Predicted FloodProbability')\nplt.show()\n\n# Residual plot\nresiduals = y - model.predict(X)\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=model.predict(X), y=residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted FloodProbability')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted FloodProbability')\nplt.show()\n\n# Histogram of residuals\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=True)\nplt.xlabel('Residuals')\nplt.title('Histogram of Residuals')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Catboost","metadata":{}},{"cell_type":"code","source":"# Train and evaluate the CatBoost model\nscores['CatBoost'], oof['CatBoost'], test_preds['CatBoost'] = score_model(\n    make_pipeline(\n        FE(use_quantile=True, sort=True),\n        CatBoostRegressor(\n            iterations=2000,\n            verbose=False,\n            random_state=config['SEED']\n        )\n    ),\n    'CatBoost'\n)\n\n# Visualize feature importance for CatBoost\nmodel = CatBoostRegressor(\n    iterations=2000,\n    verbose=False,\n    random_state=config['SEED']\n)\nmodel.fit(X, y)\n\n# CatBoost doesn't have built-in feature importance visualization like XGBoost,\n# but you can access feature importance directly from the model object\n# Custom feature importance plot for CatBoost\nimportance = model.feature_importances_\nfeature_names = X.columns\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importance}).sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x='Importance', y='Feature', data=importance_df)\nplt.title('Feature Importance (CatBoost)')\nplt.show()\n\n# Scatter plot of predicted vs actual values for CatBoost\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y, y=model.predict(X))\nplt.xlabel('Actual FloodProbability')\nplt.ylabel('Predicted FloodProbability')\nplt.title('Actual vs Predicted FloodProbability (CatBoost)')\nplt.show()\n\n# Residual plot for CatBoost\nresiduals = y - model.predict(X)\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=model.predict(X), y=residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted FloodProbability')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted FloodProbability (CatBoost)')\nplt.show()\n\n# Histogram of residuals for CatBoost\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=True)\nplt.xlabel('Residuals')\nplt.title('Histogram of Residuals (CatBoost)')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"# Train and evaluate the Linear Regression model\nscores['LinearRegression'], oof['LinearRegression'], test_preds['LinearRegression'] = score_model(\n    make_pipeline(\n        FE(use_quantile=True, sort=True),\n        LinearRegression()\n    ),\n    'LinearRegression'\n)\n\n# Linear Regression doesn't have built-in feature importance,\n# so we won't visualize feature importance\n\n# Scatter plot of predicted vs actual values for Linear Regression\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=y, y=model.predict(X))\nplt.xlabel('Actual FloodProbability')\nplt.ylabel('Predicted FloodProbability')\nplt.title('Actual vs Predicted FloodProbability (Linear Regression)')\nplt.show()\n\n# Residual plot for Linear Regression\nresiduals = y - model.predict(X)\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=model.predict(X), y=residuals)\nplt.axhline(0, color='red', linestyle='--')\nplt.xlabel('Predicted FloodProbability')\nplt.ylabel('Residuals')\nplt.title('Residuals vs Predicted FloodProbability (Linear Regression)')\nplt.show()\n\n# Histogram of residuals for Linear Regression\nplt.figure(figsize=(8, 6))\nsns.histplot(residuals, kde=True)\nplt.xlabel('Residuals')\nplt.title('Histogram of Residuals (Linear Regression)')\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming test_preds contains your predictions\nsubmission = pd.DataFrame({'id': df_sub['id'], 'FloodProbability': test_preds['CatBoost']})\n\n# Save submission to a CSV file\nsubmission.to_csv('submission_catboost.csv', index=False)\n","metadata":{},"execution_count":null,"outputs":[]}]}